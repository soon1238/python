{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = red> Tutorial: Web scraping using Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = blue>Goal:</font> retrieve stock indices automatically from the Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import urllib2  \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take one page from the Bloomberg Quote website as an example.\n",
    "\n",
    "As someone following the stock market, we would like to get the index name (S&P 500) and its price from this page. \n",
    "\n",
    "First right-click and open your browser's inspector to inspect the webpage. Try hovering your cursor on the price and you should be able to see a blue box surrounding it. Click and the related HTML will be selected in the browser console. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, we can see that the price is inside a few levels of HTML tags, which is:\n",
    "\n",
    "    <div class=\"basic-quote\"> → <div class=\"price-container up\"> → <div class=\"price\">\n",
    "  \n",
    "Similarly, if you hover and click the name \"S&P 500 Index\", it is inside:\n",
    "\n",
    "    <div class=\"basic-quote\"> and <h1 class=\"name\"> \n",
    "    \n",
    "Now we know the unique location of our data with the help of id and class (though in this case, Bloomberg do not use id)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** To start, declare a variable for the url of the page.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify the url of S&P 500\n",
    "quote_page = 'http://www.bloomberg.com/quote/SPX:IND'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Then, make use of the Python urllib2 to get the HTML page of the url declared.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query the website and return the html to the variable 'page'\n",
    "page = urllib2.urlopen(quote_page)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, parse the page into BeautifulSoup format so we can use BeautifulSoup to work on it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse the html using beautiful soap and store in variable `soup`\n",
    "soup = BeautifulSoup(page, 'html.parser')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we have a variable soup containing the HTML of the page. Here's where we can start coding the part that extracts the data.**\n",
    "\n",
    "**Remember the unique layers of our data? BeautifulSoup can help us get into these layers and extract the content out easily by using find(). In this case, since the HTML tag of name is very unique on this page, we can simple query** *div class=\"name\"* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take out the <div> of name and get its value\n",
    "name_box = soup.find('h1', attrs={'class': 'name'})  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After we have the tag, we can get the data by getting its text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S&P 500 Index\n"
     ]
    }
   ],
   "source": [
    "name = name_box.text.strip() # strip() is used to remove starting and trailing  \n",
    "print name  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarly, we can retrieve the current price of the S&P 500 index.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,414.22\n"
     ]
    }
   ],
   "source": [
    "# get the index price\n",
    "price_box = soup.find('div', attrs={'class':'price'})  \n",
    "price = price_box.text  \n",
    "print price  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = green>Store data to CSV. file </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Python csv module and the datetime module to get the record date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv  \n",
    "from datetime import datetime  \n",
    "\n",
    "# open a csv file with append, so old data will not be erased\n",
    "with open('index.csv', 'a') as csv_file:  \n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([name, price, datetime.now()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = purple> Now, try extracting multiple indices at the same time!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, modify the quote_page into an array of URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quote_page = ['http://www.bloomberg.com/quote/SPX:IND', 'http://www.bloomberg.com/quote/CCMP:IND']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we change the data extraction code into a 'for loop', which will process the URLs one by one and store all the data into a variable data in tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for loop\n",
    "data = []  \n",
    "for pg in quote_page:  \n",
    "    # query the website and return the html to the variable 'page'\n",
    "    page = urllib2.urlopen(pg)\n",
    "\n",
    "    # parse the html using beautiful soap and store in variable `soup`\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    # Take out the <div> of name and get its value\n",
    "    name_box = soup.find('h1', attrs={'class': 'name'})\n",
    "    name = name_box.text.strip() # strip() is used to remove starting and trailing\n",
    "\n",
    "    # get the index price\n",
    "    price_box = soup.find('div', attrs={'class':'price'})\n",
    "    price = price_box.text\n",
    "\n",
    "    # save the data in tuple\n",
    "    data.append((name, price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store data in csv file as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open a csv file with append, so old data will not be erased\n",
    "with open('multiple_index.csv', 'a') as csv_file:  \n",
    "    writer = csv.writer(csv_file)\n",
    "    # The for loop\n",
    "    for name, price in data:\n",
    "        writer.writerow([name, price, datetime.now()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
